{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.applications import resnet\n",
    "from keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import load_img, img_to_array, array_to_img\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting memory consumption growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'./LungCancerDataset/The IQ-OTHNCCD lung cancer dataset'\n",
    "\n",
    "categories = ['Bengin cases', 'Malignant cases', 'Normal cases']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Size Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_data = {}\n",
    "for i in categories:\n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    temp_dict = {}\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        height, width, channels = imageio.imread(filepath).shape\n",
    "        if str(height) + ' x ' + str(width) in temp_dict:\n",
    "            temp_dict[str(height) + ' x ' + str(width)] += 1 \n",
    "        else:\n",
    "            temp_dict[str(height) + ' x ' + str(width)] = 1\n",
    "    \n",
    "    size_data[i] = temp_dict\n",
    "        \n",
    "size_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categories:\n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        print(i)\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "for i in categories:\n",
    "    cnt, samples = 0, 3\n",
    "    fig, ax = plt.subplots(samples, 3, figsize=(15, 15))\n",
    "    fig.suptitle(i)\n",
    "    \n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    for curr_cnt, file in enumerate(os.listdir(path)):\n",
    "        filepath = os.path.join(path, file)\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        \n",
    "        img0 = cv2.resize(img, (img_size, img_size))\n",
    "        \n",
    "        img1 = cv2.GaussianBlur(img0, (5, 5), 0)\n",
    "        \n",
    "        ax[cnt, 0].imshow(img)\n",
    "        ax[cnt, 1].imshow(img0)\n",
    "        ax[cnt, 2].imshow(img1)\n",
    "        cnt += 1\n",
    "        if cnt == samples:\n",
    "            break\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "img_size = 256\n",
    "\n",
    "for i in categories:\n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        # preprocess here\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        data.append([img, class_num])\n",
    "        \n",
    "random.shuffle(data)\n",
    "\n",
    "X, y = [], []\n",
    "for feature, label in data:\n",
    "    X.append(feature)\n",
    "    y.append(label)\n",
    "    \n",
    "print('X length:', len(X))\n",
    "print('y counts:', Counter(y))\n",
    "\n",
    "# normalize\n",
    "X = np.array(X).reshape(-1, img_size, img_size, 1)\n",
    "X = X / 255.0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=10, stratify=y)\n",
    "\n",
    "print(len(X_train), X_train.shape)\n",
    "print(len(X_valid), X_valid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SMOTE to oversample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train), Counter(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), X_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_size*img_size*1)\n",
    "\n",
    "print(len(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before SMOTE:', Counter(y_train))\n",
    "smote = SMOTE()\n",
    "X_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)\n",
    "print('After SMOTE:', Counter(y_train_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_size, img_size, 1)\n",
    "X_train_sampled = X_train_sampled.reshape(X_train_sampled.shape[0], img_size, img_size, 1)\n",
    "\n",
    "print(len(X_train), X_train.shape)\n",
    "print(len(X_train_sampled), X_train_sampled.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building with SMOTE data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), kernel_regularizer=regularizers.l2(0.01), input_shape=X_train.shape[1:]))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(16))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train_sampled, y_train_sampled, batch_size=8, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_valid, verbose=1)\n",
    "# y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# print(classification_report(y_valid, y_pred_bool))\n",
    "\n",
    "# print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'], label='Train')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history.history['loss'], label='Train')\n",
    "# plt.plot(history.history['val_loss'], label='Validation')\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add convolutional layers with L2 regularization\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=X_train.shape[1:]))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer with L2 regularization\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Add dropout layer\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "# Add the output layer\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train_sampled, y_train_sampled, epochs=10, batch_size=8, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_valid, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_bool))\n",
    "print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
